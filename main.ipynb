{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# things we need for NLP\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/danielkrasovski/tensorflow_macos_venv/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Scipy not supported!\n"
     ]
    }
   ],
   "source": [
    "# things we need for Tensorflow\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# import our chat-bot intents file\n",
    "import json\n",
    "with open('data/intents.json') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "files = glob.glob('models/*') #delete the models folder content\n",
    "for f in files:\n",
    "    os.remove(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 documents\n",
      "2 classes ['Arabic:', 'Easter Eggs']\n",
      "3 unique stemmed words ['arab', 'east', 'eg']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        # add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% create our training data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/danielkrasovski/tensorflow_macos_venv/lib/python3.8/site-packages/tflearn/initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "---------------------------------\n",
      "Run id: H45FZ4\n",
      "Log directory: tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 3\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 0.070s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 2  | total loss: \u001B[1m\u001B[32m0.62386\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 002 | loss: 0.62386 - acc: 0.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 3  | total loss: \u001B[1m\u001B[32m0.68028\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 003 | loss: 0.68028 - acc: 0.5455 -- iter: 3/3\n",
      "--\n",
      "Training Step: 4  | total loss: \u001B[1m\u001B[32m0.68941\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 004 | loss: 0.68941 - acc: 0.6364 -- iter: 3/3\n",
      "--\n",
      "Training Step: 5  | total loss: \u001B[1m\u001B[32m0.69126\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 005 | loss: 0.69126 - acc: 0.6573 -- iter: 3/3\n",
      "--\n",
      "Training Step: 6  | total loss: \u001B[1m\u001B[32m0.69156\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 006 | loss: 0.69156 - acc: 0.6633 -- iter: 3/3\n",
      "--\n",
      "Training Step: 7  | total loss: \u001B[1m\u001B[32m0.69144\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 007 | loss: 0.69144 - acc: 0.6653 -- iter: 3/3\n",
      "--\n",
      "Training Step: 8  | total loss: \u001B[1m\u001B[32m0.69118\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 008 | loss: 0.69118 - acc: 0.6661 -- iter: 3/3\n",
      "--\n",
      "Training Step: 9  | total loss: \u001B[1m\u001B[32m0.69088\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 009 | loss: 0.69088 - acc: 0.6664 -- iter: 3/3\n",
      "--\n",
      "Training Step: 10  | total loss: \u001B[1m\u001B[32m0.69055\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 010 | loss: 0.69055 - acc: 0.6665 -- iter: 3/3\n",
      "--\n",
      "Training Step: 11  | total loss: \u001B[1m\u001B[32m0.69021\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 011 | loss: 0.69021 - acc: 0.6666 -- iter: 3/3\n",
      "--\n",
      "Training Step: 12  | total loss: \u001B[1m\u001B[32m0.68986\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 012 | loss: 0.68986 - acc: 0.6666 -- iter: 3/3\n",
      "--\n",
      "Training Step: 13  | total loss: \u001B[1m\u001B[32m0.68951\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 013 | loss: 0.68951 - acc: 0.6666 -- iter: 3/3\n",
      "--\n",
      "Training Step: 14  | total loss: \u001B[1m\u001B[32m0.68914\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 014 | loss: 0.68914 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 15  | total loss: \u001B[1m\u001B[32m0.68877\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 015 | loss: 0.68877 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 16  | total loss: \u001B[1m\u001B[32m0.68838\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 016 | loss: 0.68838 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 17  | total loss: \u001B[1m\u001B[32m0.68799\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 017 | loss: 0.68799 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 18  | total loss: \u001B[1m\u001B[32m0.68758\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 018 | loss: 0.68758 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 19  | total loss: \u001B[1m\u001B[32m0.68716\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 019 | loss: 0.68716 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 20  | total loss: \u001B[1m\u001B[32m0.68672\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 020 | loss: 0.68672 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 21  | total loss: \u001B[1m\u001B[32m0.68628\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 021 | loss: 0.68628 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 22  | total loss: \u001B[1m\u001B[32m0.68581\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 022 | loss: 0.68581 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 23  | total loss: \u001B[1m\u001B[32m0.68533\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 023 | loss: 0.68533 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 24  | total loss: \u001B[1m\u001B[32m0.68483\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 024 | loss: 0.68483 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 25  | total loss: \u001B[1m\u001B[32m0.68431\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 025 | loss: 0.68431 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 26  | total loss: \u001B[1m\u001B[32m0.68377\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 026 | loss: 0.68377 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 27  | total loss: \u001B[1m\u001B[32m0.68321\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 027 | loss: 0.68321 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 28  | total loss: \u001B[1m\u001B[32m0.68262\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 028 | loss: 0.68262 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 29  | total loss: \u001B[1m\u001B[32m0.68201\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 029 | loss: 0.68201 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 30  | total loss: \u001B[1m\u001B[32m0.68137\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 030 | loss: 0.68137 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 31  | total loss: \u001B[1m\u001B[32m0.68070\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 031 | loss: 0.68070 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 32  | total loss: \u001B[1m\u001B[32m0.68000\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 032 | loss: 0.68000 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 33  | total loss: \u001B[1m\u001B[32m0.67927\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 033 | loss: 0.67927 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 34  | total loss: \u001B[1m\u001B[32m0.67851\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 034 | loss: 0.67851 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 35  | total loss: \u001B[1m\u001B[32m0.67772\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 035 | loss: 0.67772 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 36  | total loss: \u001B[1m\u001B[32m0.67688\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 036 | loss: 0.67688 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 37  | total loss: \u001B[1m\u001B[32m0.67601\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 037 | loss: 0.67601 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 38  | total loss: \u001B[1m\u001B[32m0.67510\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 038 | loss: 0.67510 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 39  | total loss: \u001B[1m\u001B[32m0.67414\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 039 | loss: 0.67414 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 40  | total loss: \u001B[1m\u001B[32m0.67314\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 040 | loss: 0.67314 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 41  | total loss: \u001B[1m\u001B[32m0.67210\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 041 | loss: 0.67210 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 42  | total loss: \u001B[1m\u001B[32m0.67101\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 042 | loss: 0.67101 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 43  | total loss: \u001B[1m\u001B[32m0.66987\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 043 | loss: 0.66987 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 44  | total loss: \u001B[1m\u001B[32m0.66867\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 044 | loss: 0.66867 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 45  | total loss: \u001B[1m\u001B[32m0.66743\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 045 | loss: 0.66743 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 46  | total loss: \u001B[1m\u001B[32m0.66612\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 046 | loss: 0.66612 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 47  | total loss: \u001B[1m\u001B[32m0.66476\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 047 | loss: 0.66476 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 48  | total loss: \u001B[1m\u001B[32m0.66334\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 048 | loss: 0.66334 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 49  | total loss: \u001B[1m\u001B[32m0.66186\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 049 | loss: 0.66186 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 50  | total loss: \u001B[1m\u001B[32m0.66031\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 050 | loss: 0.66031 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 51  | total loss: \u001B[1m\u001B[32m0.65870\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 051 | loss: 0.65870 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 52  | total loss: \u001B[1m\u001B[32m0.65703\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 052 | loss: 0.65703 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 53  | total loss: \u001B[1m\u001B[32m0.65528\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 053 | loss: 0.65528 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 54  | total loss: \u001B[1m\u001B[32m0.65346\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 054 | loss: 0.65346 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 55  | total loss: \u001B[1m\u001B[32m0.65157\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 055 | loss: 0.65157 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 56  | total loss: \u001B[1m\u001B[32m0.64960\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 056 | loss: 0.64960 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 57  | total loss: \u001B[1m\u001B[32m0.64756\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 057 | loss: 0.64756 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 58  | total loss: \u001B[1m\u001B[32m0.64544\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 058 | loss: 0.64544 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 59  | total loss: \u001B[1m\u001B[32m0.64324\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 059 | loss: 0.64324 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 60  | total loss: \u001B[1m\u001B[32m0.64096\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 060 | loss: 0.64096 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 61  | total loss: \u001B[1m\u001B[32m0.63860\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 061 | loss: 0.63860 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 62  | total loss: \u001B[1m\u001B[32m0.63615\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 062 | loss: 0.63615 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 63  | total loss: \u001B[1m\u001B[32m0.63362\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 063 | loss: 0.63362 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 64  | total loss: \u001B[1m\u001B[32m0.63100\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 064 | loss: 0.63100 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 65  | total loss: \u001B[1m\u001B[32m0.62829\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 065 | loss: 0.62829 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 66  | total loss: \u001B[1m\u001B[32m0.62549\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 066 | loss: 0.62549 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 67  | total loss: \u001B[1m\u001B[32m0.62260\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 067 | loss: 0.62260 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 68  | total loss: \u001B[1m\u001B[32m0.61963\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 068 | loss: 0.61963 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 69  | total loss: \u001B[1m\u001B[32m0.61656\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 069 | loss: 0.61656 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 70  | total loss: \u001B[1m\u001B[32m0.61339\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 070 | loss: 0.61339 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 71  | total loss: \u001B[1m\u001B[32m0.61014\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 071 | loss: 0.61014 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 72  | total loss: \u001B[1m\u001B[32m0.60679\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 072 | loss: 0.60679 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 73  | total loss: \u001B[1m\u001B[32m0.60335\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 073 | loss: 0.60335 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 74  | total loss: \u001B[1m\u001B[32m0.59982\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 074 | loss: 0.59982 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 75  | total loss: \u001B[1m\u001B[32m0.59619\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 075 | loss: 0.59619 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 76  | total loss: \u001B[1m\u001B[32m0.59247\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 076 | loss: 0.59247 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 77  | total loss: \u001B[1m\u001B[32m0.58865\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 077 | loss: 0.58865 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 78  | total loss: \u001B[1m\u001B[32m0.58474\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 078 | loss: 0.58474 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 79  | total loss: \u001B[1m\u001B[32m0.58074\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 079 | loss: 0.58074 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 80  | total loss: \u001B[1m\u001B[32m0.57665\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 080 | loss: 0.57665 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 81  | total loss: \u001B[1m\u001B[32m0.57247\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 081 | loss: 0.57247 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 82  | total loss: \u001B[1m\u001B[32m0.56820\u001B[0m\u001B[0m | time: 0.005s\n",
      "| Adam | epoch: 082 | loss: 0.56820 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 83  | total loss: \u001B[1m\u001B[32m0.56379\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 083 | loss: 0.56379 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 84  | total loss: \u001B[1m\u001B[32m0.55925\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 084 | loss: 0.55925 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 85  | total loss: \u001B[1m\u001B[32m0.55457\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 085 | loss: 0.55457 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 86  | total loss: \u001B[1m\u001B[32m0.54978\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 086 | loss: 0.54978 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 87  | total loss: \u001B[1m\u001B[32m0.54486\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 087 | loss: 0.54486 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 88  | total loss: \u001B[1m\u001B[32m0.53982\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 088 | loss: 0.53982 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 89  | total loss: \u001B[1m\u001B[32m0.53467\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 089 | loss: 0.53467 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 90  | total loss: \u001B[1m\u001B[32m0.52941\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 090 | loss: 0.52941 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 91  | total loss: \u001B[1m\u001B[32m0.52404\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 091 | loss: 0.52404 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 92  | total loss: \u001B[1m\u001B[32m0.51857\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 092 | loss: 0.51857 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 93  | total loss: \u001B[1m\u001B[32m0.51300\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 093 | loss: 0.51300 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 94  | total loss: \u001B[1m\u001B[32m0.50734\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 094 | loss: 0.50734 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 95  | total loss: \u001B[1m\u001B[32m0.50158\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 095 | loss: 0.50158 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 96  | total loss: \u001B[1m\u001B[32m0.49573\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 096 | loss: 0.49573 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 97  | total loss: \u001B[1m\u001B[32m0.48980\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 097 | loss: 0.48980 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 98  | total loss: \u001B[1m\u001B[32m0.48378\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 098 | loss: 0.48378 - acc: 0.6667 -- iter: 3/3\n",
      "--\n",
      "Training Step: 99  | total loss: \u001B[1m\u001B[32m0.47769\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 099 | loss: 0.47769 - acc: 0.7000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 100  | total loss: \u001B[1m\u001B[32m0.47152\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 100 | loss: 0.47152 - acc: 0.7300 -- iter: 3/3\n",
      "--\n",
      "Training Step: 101  | total loss: \u001B[1m\u001B[32m0.46527\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 101 | loss: 0.46527 - acc: 0.7570 -- iter: 3/3\n",
      "--\n",
      "Training Step: 102  | total loss: \u001B[1m\u001B[32m0.45896\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 102 | loss: 0.45896 - acc: 0.7813 -- iter: 3/3\n",
      "--\n",
      "Training Step: 103  | total loss: \u001B[1m\u001B[32m0.45259\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 103 | loss: 0.45259 - acc: 0.8032 -- iter: 3/3\n",
      "--\n",
      "Training Step: 104  | total loss: \u001B[1m\u001B[32m0.44615\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 104 | loss: 0.44615 - acc: 0.8229 -- iter: 3/3\n",
      "--\n",
      "Training Step: 105  | total loss: \u001B[1m\u001B[32m0.43964\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 105 | loss: 0.43964 - acc: 0.8406 -- iter: 3/3\n",
      "--\n",
      "Training Step: 106  | total loss: \u001B[1m\u001B[32m0.43309\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 106 | loss: 0.43309 - acc: 0.8565 -- iter: 3/3\n",
      "--\n",
      "Training Step: 107  | total loss: \u001B[1m\u001B[32m0.42648\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 107 | loss: 0.42648 - acc: 0.8709 -- iter: 3/3\n",
      "--\n",
      "Training Step: 108  | total loss: \u001B[1m\u001B[32m0.41981\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 108 | loss: 0.41981 - acc: 0.8838 -- iter: 3/3\n",
      "--\n",
      "Training Step: 109  | total loss: \u001B[1m\u001B[32m0.41310\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 109 | loss: 0.41310 - acc: 0.8954 -- iter: 3/3\n",
      "--\n",
      "Training Step: 110  | total loss: \u001B[1m\u001B[32m0.40635\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 110 | loss: 0.40635 - acc: 0.9059 -- iter: 3/3\n",
      "--\n",
      "Training Step: 111  | total loss: \u001B[1m\u001B[32m0.39955\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 111 | loss: 0.39955 - acc: 0.9153 -- iter: 3/3\n",
      "--\n",
      "Training Step: 112  | total loss: \u001B[1m\u001B[32m0.39271\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 112 | loss: 0.39271 - acc: 0.9237 -- iter: 3/3\n",
      "--\n",
      "Training Step: 113  | total loss: \u001B[1m\u001B[32m0.38584\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 113 | loss: 0.38584 - acc: 0.9314 -- iter: 3/3\n",
      "--\n",
      "Training Step: 114  | total loss: \u001B[1m\u001B[32m0.37894\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 114 | loss: 0.37894 - acc: 0.9382 -- iter: 3/3\n",
      "--\n",
      "Training Step: 115  | total loss: \u001B[1m\u001B[32m0.37201\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 115 | loss: 0.37201 - acc: 0.9444 -- iter: 3/3\n",
      "--\n",
      "Training Step: 116  | total loss: \u001B[1m\u001B[32m0.36506\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 116 | loss: 0.36506 - acc: 0.9500 -- iter: 3/3\n",
      "--\n",
      "Training Step: 117  | total loss: \u001B[1m\u001B[32m0.35808\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 117 | loss: 0.35808 - acc: 0.9550 -- iter: 3/3\n",
      "--\n",
      "Training Step: 118  | total loss: \u001B[1m\u001B[32m0.35110\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 118 | loss: 0.35110 - acc: 0.9595 -- iter: 3/3\n",
      "--\n",
      "Training Step: 119  | total loss: \u001B[1m\u001B[32m0.34410\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 119 | loss: 0.34410 - acc: 0.9635 -- iter: 3/3\n",
      "--\n",
      "Training Step: 120  | total loss: \u001B[1m\u001B[32m0.33709\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 120 | loss: 0.33709 - acc: 0.9672 -- iter: 3/3\n",
      "--\n",
      "Training Step: 121  | total loss: \u001B[1m\u001B[32m0.33009\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 121 | loss: 0.33009 - acc: 0.9705 -- iter: 3/3\n",
      "--\n",
      "Training Step: 122  | total loss: \u001B[1m\u001B[32m0.32310\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 122 | loss: 0.32310 - acc: 0.9734 -- iter: 3/3\n",
      "--\n",
      "Training Step: 123  | total loss: \u001B[1m\u001B[32m0.31611\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 123 | loss: 0.31611 - acc: 0.9761 -- iter: 3/3\n",
      "--\n",
      "Training Step: 124  | total loss: \u001B[1m\u001B[32m0.30914\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 124 | loss: 0.30914 - acc: 0.9785 -- iter: 3/3\n",
      "--\n",
      "Training Step: 125  | total loss: \u001B[1m\u001B[32m0.30220\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 125 | loss: 0.30220 - acc: 0.9806 -- iter: 3/3\n",
      "--\n",
      "Training Step: 126  | total loss: \u001B[1m\u001B[32m0.29529\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 126 | loss: 0.29529 - acc: 0.9826 -- iter: 3/3\n",
      "--\n",
      "Training Step: 127  | total loss: \u001B[1m\u001B[32m0.28841\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 127 | loss: 0.28841 - acc: 0.9843 -- iter: 3/3\n",
      "--\n",
      "Training Step: 128  | total loss: \u001B[1m\u001B[32m0.28158\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 128 | loss: 0.28158 - acc: 0.9859 -- iter: 3/3\n",
      "--\n",
      "Training Step: 129  | total loss: \u001B[1m\u001B[32m0.27479\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 129 | loss: 0.27479 - acc: 0.9873 -- iter: 3/3\n",
      "--\n",
      "Training Step: 130  | total loss: \u001B[1m\u001B[32m0.26806\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 130 | loss: 0.26806 - acc: 0.9886 -- iter: 3/3\n",
      "--\n",
      "Training Step: 131  | total loss: \u001B[1m\u001B[32m0.26139\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 131 | loss: 0.26139 - acc: 0.9897 -- iter: 3/3\n",
      "--\n",
      "Training Step: 132  | total loss: \u001B[1m\u001B[32m0.25479\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 132 | loss: 0.25479 - acc: 0.9907 -- iter: 3/3\n",
      "--\n",
      "Training Step: 133  | total loss: \u001B[1m\u001B[32m0.24826\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 133 | loss: 0.24826 - acc: 0.9917 -- iter: 3/3\n",
      "--\n",
      "Training Step: 134  | total loss: \u001B[1m\u001B[32m0.24181\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 134 | loss: 0.24181 - acc: 0.9925 -- iter: 3/3\n",
      "--\n",
      "Training Step: 135  | total loss: \u001B[1m\u001B[32m0.23544\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 135 | loss: 0.23544 - acc: 0.9932 -- iter: 3/3\n",
      "--\n",
      "Training Step: 136  | total loss: \u001B[1m\u001B[32m0.22916\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 136 | loss: 0.22916 - acc: 0.9939 -- iter: 3/3\n",
      "--\n",
      "Training Step: 137  | total loss: \u001B[1m\u001B[32m0.22298\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 137 | loss: 0.22298 - acc: 0.9945 -- iter: 3/3\n",
      "--\n",
      "Training Step: 138  | total loss: \u001B[1m\u001B[32m0.21689\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 138 | loss: 0.21689 - acc: 0.9951 -- iter: 3/3\n",
      "--\n",
      "Training Step: 139  | total loss: \u001B[1m\u001B[32m0.21091\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 139 | loss: 0.21091 - acc: 0.9956 -- iter: 3/3\n",
      "--\n",
      "Training Step: 140  | total loss: \u001B[1m\u001B[32m0.20503\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 140 | loss: 0.20503 - acc: 0.9960 -- iter: 3/3\n",
      "--\n",
      "Training Step: 141  | total loss: \u001B[1m\u001B[32m0.19926\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 141 | loss: 0.19926 - acc: 0.9964 -- iter: 3/3\n",
      "--\n",
      "Training Step: 142  | total loss: \u001B[1m\u001B[32m0.19361\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 142 | loss: 0.19361 - acc: 0.9968 -- iter: 3/3\n",
      "--\n",
      "Training Step: 143  | total loss: \u001B[1m\u001B[32m0.18806\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 143 | loss: 0.18806 - acc: 0.9971 -- iter: 3/3\n",
      "--\n",
      "Training Step: 144  | total loss: \u001B[1m\u001B[32m0.18264\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 144 | loss: 0.18264 - acc: 0.9974 -- iter: 3/3\n",
      "--\n",
      "Training Step: 145  | total loss: \u001B[1m\u001B[32m0.17733\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 145 | loss: 0.17733 - acc: 0.9976 -- iter: 3/3\n",
      "--\n",
      "Training Step: 146  | total loss: \u001B[1m\u001B[32m0.17215\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 146 | loss: 0.17215 - acc: 0.9979 -- iter: 3/3\n",
      "--\n",
      "Training Step: 147  | total loss: \u001B[1m\u001B[32m0.16708\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 147 | loss: 0.16708 - acc: 0.9981 -- iter: 3/3\n",
      "--\n",
      "Training Step: 148  | total loss: \u001B[1m\u001B[32m0.16214\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 148 | loss: 0.16214 - acc: 0.9983 -- iter: 3/3\n",
      "--\n",
      "Training Step: 149  | total loss: \u001B[1m\u001B[32m0.15732\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 149 | loss: 0.15732 - acc: 0.9985 -- iter: 3/3\n",
      "--\n",
      "Training Step: 150  | total loss: \u001B[1m\u001B[32m0.15262\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 150 | loss: 0.15262 - acc: 0.9986 -- iter: 3/3\n",
      "--\n",
      "Training Step: 151  | total loss: \u001B[1m\u001B[32m0.14805\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 151 | loss: 0.14805 - acc: 0.9987 -- iter: 3/3\n",
      "--\n",
      "Training Step: 152  | total loss: \u001B[1m\u001B[32m0.14359\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 152 | loss: 0.14359 - acc: 0.9989 -- iter: 3/3\n",
      "--\n",
      "Training Step: 153  | total loss: \u001B[1m\u001B[32m0.13926\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 153 | loss: 0.13926 - acc: 0.9990 -- iter: 3/3\n",
      "--\n",
      "Training Step: 154  | total loss: \u001B[1m\u001B[32m0.13505\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 154 | loss: 0.13505 - acc: 0.9991 -- iter: 3/3\n",
      "--\n",
      "Training Step: 155  | total loss: \u001B[1m\u001B[32m0.13095\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 155 | loss: 0.13095 - acc: 0.9992 -- iter: 3/3\n",
      "--\n",
      "Training Step: 156  | total loss: \u001B[1m\u001B[32m0.12698\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 156 | loss: 0.12698 - acc: 0.9993 -- iter: 3/3\n",
      "--\n",
      "Training Step: 157  | total loss: \u001B[1m\u001B[32m0.12312\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 157 | loss: 0.12312 - acc: 0.9993 -- iter: 3/3\n",
      "--\n",
      "Training Step: 158  | total loss: \u001B[1m\u001B[32m0.27608\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 158 | loss: 0.27608 - acc: 0.9327 -- iter: 3/3\n",
      "--\n",
      "Training Step: 159  | total loss: \u001B[1m\u001B[32m0.25697\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 159 | loss: 0.25697 - acc: 0.9395 -- iter: 3/3\n",
      "--\n",
      "Training Step: 160  | total loss: \u001B[1m\u001B[32m0.23968\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 160 | loss: 0.23968 - acc: 0.9455 -- iter: 3/3\n",
      "--\n",
      "Training Step: 161  | total loss: \u001B[1m\u001B[32m0.22403\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 161 | loss: 0.22403 - acc: 0.9510 -- iter: 3/3\n",
      "--\n",
      "Training Step: 162  | total loss: \u001B[1m\u001B[32m0.20984\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 162 | loss: 0.20984 - acc: 0.9559 -- iter: 3/3\n",
      "--\n",
      "Training Step: 163  | total loss: \u001B[1m\u001B[32m0.19695\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 163 | loss: 0.19695 - acc: 0.9603 -- iter: 3/3\n",
      "--\n",
      "Training Step: 164  | total loss: \u001B[1m\u001B[32m0.18524\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 164 | loss: 0.18524 - acc: 0.9643 -- iter: 3/3\n",
      "--\n",
      "Training Step: 165  | total loss: \u001B[1m\u001B[32m0.17458\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 165 | loss: 0.17458 - acc: 0.9678 -- iter: 3/3\n",
      "--\n",
      "Training Step: 166  | total loss: \u001B[1m\u001B[32m0.16485\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 166 | loss: 0.16485 - acc: 0.9710 -- iter: 3/3\n",
      "--\n",
      "Training Step: 167  | total loss: \u001B[1m\u001B[32m0.15597\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 167 | loss: 0.15597 - acc: 0.9739 -- iter: 3/3\n",
      "--\n",
      "Training Step: 168  | total loss: \u001B[1m\u001B[32m0.14784\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 168 | loss: 0.14784 - acc: 0.9765 -- iter: 3/3\n",
      "--\n",
      "Training Step: 169  | total loss: \u001B[1m\u001B[32m0.14038\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 169 | loss: 0.14038 - acc: 0.9789 -- iter: 3/3\n",
      "--\n",
      "Training Step: 170  | total loss: \u001B[1m\u001B[32m0.13354\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 170 | loss: 0.13354 - acc: 0.9810 -- iter: 3/3\n",
      "--\n",
      "Training Step: 171  | total loss: \u001B[1m\u001B[32m0.12724\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 171 | loss: 0.12724 - acc: 0.9829 -- iter: 3/3\n",
      "--\n",
      "Training Step: 172  | total loss: \u001B[1m\u001B[32m0.12144\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 172 | loss: 0.12144 - acc: 0.9846 -- iter: 3/3\n",
      "--\n",
      "Training Step: 173  | total loss: \u001B[1m\u001B[32m0.11608\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 173 | loss: 0.11608 - acc: 0.9862 -- iter: 3/3\n",
      "--\n",
      "Training Step: 174  | total loss: \u001B[1m\u001B[32m0.11112\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 174 | loss: 0.11112 - acc: 0.9875 -- iter: 3/3\n",
      "--\n",
      "Training Step: 175  | total loss: \u001B[1m\u001B[32m0.10652\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 175 | loss: 0.10652 - acc: 0.9888 -- iter: 3/3\n",
      "--\n",
      "Training Step: 176  | total loss: \u001B[1m\u001B[32m0.10224\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 176 | loss: 0.10224 - acc: 0.9899 -- iter: 3/3\n",
      "--\n",
      "Training Step: 177  | total loss: \u001B[1m\u001B[32m0.09826\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 177 | loss: 0.09826 - acc: 0.9909 -- iter: 3/3\n",
      "--\n",
      "Training Step: 178  | total loss: \u001B[1m\u001B[32m0.09455\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 178 | loss: 0.09455 - acc: 0.9918 -- iter: 3/3\n",
      "--\n",
      "Training Step: 179  | total loss: \u001B[1m\u001B[32m0.09108\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 179 | loss: 0.09108 - acc: 0.9926 -- iter: 3/3\n",
      "--\n",
      "Training Step: 180  | total loss: \u001B[1m\u001B[32m0.08783\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 180 | loss: 0.08783 - acc: 0.9934 -- iter: 3/3\n",
      "--\n",
      "Training Step: 181  | total loss: \u001B[1m\u001B[32m0.08478\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 181 | loss: 0.08478 - acc: 0.9940 -- iter: 3/3\n",
      "--\n",
      "Training Step: 182  | total loss: \u001B[1m\u001B[32m0.08191\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 182 | loss: 0.08191 - acc: 0.9946 -- iter: 3/3\n",
      "--\n",
      "Training Step: 183  | total loss: \u001B[1m\u001B[32m0.07921\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 183 | loss: 0.07921 - acc: 0.9952 -- iter: 3/3\n",
      "--\n",
      "Training Step: 184  | total loss: \u001B[1m\u001B[32m0.07666\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 184 | loss: 0.07666 - acc: 0.9957 -- iter: 3/3\n",
      "--\n",
      "Training Step: 185  | total loss: \u001B[1m\u001B[32m0.07425\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 185 | loss: 0.07425 - acc: 0.9961 -- iter: 3/3\n",
      "--\n",
      "Training Step: 186  | total loss: \u001B[1m\u001B[32m0.07197\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 186 | loss: 0.07197 - acc: 0.9965 -- iter: 3/3\n",
      "--\n",
      "Training Step: 187  | total loss: \u001B[1m\u001B[32m0.06981\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 187 | loss: 0.06981 - acc: 0.9968 -- iter: 3/3\n",
      "--\n",
      "Training Step: 188  | total loss: \u001B[1m\u001B[32m0.06776\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 188 | loss: 0.06776 - acc: 0.9971 -- iter: 3/3\n",
      "--\n",
      "Training Step: 189  | total loss: \u001B[1m\u001B[32m0.06580\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 189 | loss: 0.06580 - acc: 0.9974 -- iter: 3/3\n",
      "--\n",
      "Training Step: 190  | total loss: \u001B[1m\u001B[32m0.06395\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 190 | loss: 0.06395 - acc: 0.9977 -- iter: 3/3\n",
      "--\n",
      "Training Step: 191  | total loss: \u001B[1m\u001B[32m0.06217\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 191 | loss: 0.06217 - acc: 0.9979 -- iter: 3/3\n",
      "--\n",
      "Training Step: 192  | total loss: \u001B[1m\u001B[32m0.06048\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 192 | loss: 0.06048 - acc: 0.9981 -- iter: 3/3\n",
      "--\n",
      "Training Step: 193  | total loss: \u001B[1m\u001B[32m0.05886\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 193 | loss: 0.05886 - acc: 0.9983 -- iter: 3/3\n",
      "--\n",
      "Training Step: 194  | total loss: \u001B[1m\u001B[32m0.05731\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 194 | loss: 0.05731 - acc: 0.9985 -- iter: 3/3\n",
      "--\n",
      "Training Step: 195  | total loss: \u001B[1m\u001B[32m0.05583\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 195 | loss: 0.05583 - acc: 0.9986 -- iter: 3/3\n",
      "--\n",
      "Training Step: 196  | total loss: \u001B[1m\u001B[32m0.05441\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 196 | loss: 0.05441 - acc: 0.9988 -- iter: 3/3\n",
      "--\n",
      "Training Step: 197  | total loss: \u001B[1m\u001B[32m0.05305\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 197 | loss: 0.05305 - acc: 0.9989 -- iter: 3/3\n",
      "--\n",
      "Training Step: 198  | total loss: \u001B[1m\u001B[32m0.05173\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 198 | loss: 0.05173 - acc: 0.9990 -- iter: 3/3\n",
      "--\n",
      "Training Step: 199  | total loss: \u001B[1m\u001B[32m0.05047\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 199 | loss: 0.05047 - acc: 0.9991 -- iter: 3/3\n",
      "--\n",
      "Training Step: 200  | total loss: \u001B[1m\u001B[32m0.04926\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 200 | loss: 0.04926 - acc: 0.9992 -- iter: 3/3\n",
      "--\n",
      "Training Step: 201  | total loss: \u001B[1m\u001B[32m0.04809\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 201 | loss: 0.04809 - acc: 0.9993 -- iter: 3/3\n",
      "--\n",
      "Training Step: 202  | total loss: \u001B[1m\u001B[32m0.04697\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 202 | loss: 0.04697 - acc: 0.9993 -- iter: 3/3\n",
      "--\n",
      "Training Step: 203  | total loss: \u001B[1m\u001B[32m0.04588\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 203 | loss: 0.04588 - acc: 0.9994 -- iter: 3/3\n",
      "--\n",
      "Training Step: 204  | total loss: \u001B[1m\u001B[32m0.04484\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 204 | loss: 0.04484 - acc: 0.9995 -- iter: 3/3\n",
      "--\n",
      "Training Step: 205  | total loss: \u001B[1m\u001B[32m0.04383\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 205 | loss: 0.04383 - acc: 0.9995 -- iter: 3/3\n",
      "--\n",
      "Training Step: 206  | total loss: \u001B[1m\u001B[32m0.04285\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 206 | loss: 0.04285 - acc: 0.9996 -- iter: 3/3\n",
      "--\n",
      "Training Step: 207  | total loss: \u001B[1m\u001B[32m0.04191\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 207 | loss: 0.04191 - acc: 0.9996 -- iter: 3/3\n",
      "--\n",
      "Training Step: 208  | total loss: \u001B[1m\u001B[32m0.04100\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 208 | loss: 0.04100 - acc: 0.9997 -- iter: 3/3\n",
      "--\n",
      "Training Step: 209  | total loss: \u001B[1m\u001B[32m0.04011\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 209 | loss: 0.04011 - acc: 0.9997 -- iter: 3/3\n",
      "--\n",
      "Training Step: 210  | total loss: \u001B[1m\u001B[32m0.03926\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 210 | loss: 0.03926 - acc: 0.9997 -- iter: 3/3\n",
      "--\n",
      "Training Step: 211  | total loss: \u001B[1m\u001B[32m0.03843\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 211 | loss: 0.03843 - acc: 0.9997 -- iter: 3/3\n",
      "--\n",
      "Training Step: 212  | total loss: \u001B[1m\u001B[32m0.26540\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 212 | loss: 0.26540 - acc: 0.9331 -- iter: 3/3\n",
      "--\n",
      "Training Step: 213  | total loss: \u001B[1m\u001B[32m0.24195\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 213 | loss: 0.24195 - acc: 0.9398 -- iter: 3/3\n",
      "--\n",
      "Training Step: 214  | total loss: \u001B[1m\u001B[32m0.22088\u001B[0m\u001B[0m | time: 0.002s\n",
      "| Adam | epoch: 214 | loss: 0.22088 - acc: 0.9458 -- iter: 3/3\n",
      "--\n",
      "Training Step: 215  | total loss: \u001B[1m\u001B[32m0.20194\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 215 | loss: 0.20194 - acc: 0.9512 -- iter: 3/3\n",
      "--\n",
      "Training Step: 216  | total loss: \u001B[1m\u001B[32m0.18492\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 216 | loss: 0.18492 - acc: 0.9561 -- iter: 3/3\n",
      "--\n",
      "Training Step: 217  | total loss: \u001B[1m\u001B[32m0.16962\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 217 | loss: 0.16962 - acc: 0.9605 -- iter: 3/3\n",
      "--\n",
      "Training Step: 218  | total loss: \u001B[1m\u001B[32m0.15585\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 218 | loss: 0.15585 - acc: 0.9644 -- iter: 3/3\n",
      "--\n",
      "Training Step: 219  | total loss: \u001B[1m\u001B[32m0.14347\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 219 | loss: 0.14347 - acc: 0.9680 -- iter: 3/3\n",
      "--\n",
      "Training Step: 220  | total loss: \u001B[1m\u001B[32m0.13232\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 220 | loss: 0.13232 - acc: 0.9712 -- iter: 3/3\n",
      "--\n",
      "Training Step: 221  | total loss: \u001B[1m\u001B[32m0.12228\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 221 | loss: 0.12228 - acc: 0.9741 -- iter: 3/3\n",
      "--\n",
      "Training Step: 222  | total loss: \u001B[1m\u001B[32m0.11324\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 222 | loss: 0.11324 - acc: 0.9767 -- iter: 3/3\n",
      "--\n",
      "Training Step: 223  | total loss: \u001B[1m\u001B[32m0.10509\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 223 | loss: 0.10509 - acc: 0.9790 -- iter: 3/3\n",
      "--\n",
      "Training Step: 224  | total loss: \u001B[1m\u001B[32m0.09774\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 224 | loss: 0.09774 - acc: 0.9811 -- iter: 3/3\n",
      "--\n",
      "Training Step: 225  | total loss: \u001B[1m\u001B[32m0.09110\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 225 | loss: 0.09110 - acc: 0.9830 -- iter: 3/3\n",
      "--\n",
      "Training Step: 226  | total loss: \u001B[1m\u001B[32m0.08511\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 226 | loss: 0.08511 - acc: 0.9847 -- iter: 3/3\n",
      "--\n",
      "Training Step: 227  | total loss: \u001B[1m\u001B[32m0.07969\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 227 | loss: 0.07969 - acc: 0.9862 -- iter: 3/3\n",
      "--\n",
      "Training Step: 228  | total loss: \u001B[1m\u001B[32m0.07479\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 228 | loss: 0.07479 - acc: 0.9876 -- iter: 3/3\n",
      "--\n",
      "Training Step: 229  | total loss: \u001B[1m\u001B[32m0.07035\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 229 | loss: 0.07035 - acc: 0.9888 -- iter: 3/3\n",
      "--\n",
      "Training Step: 230  | total loss: \u001B[1m\u001B[32m0.06633\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 230 | loss: 0.06633 - acc: 0.9900 -- iter: 3/3\n",
      "--\n",
      "Training Step: 231  | total loss: \u001B[1m\u001B[32m0.06268\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 231 | loss: 0.06268 - acc: 0.9910 -- iter: 3/3\n",
      "--\n",
      "Training Step: 232  | total loss: \u001B[1m\u001B[32m0.05937\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 232 | loss: 0.05937 - acc: 0.9919 -- iter: 3/3\n",
      "--\n",
      "Training Step: 233  | total loss: \u001B[1m\u001B[32m0.05635\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 233 | loss: 0.05635 - acc: 0.9927 -- iter: 3/3\n",
      "--\n",
      "Training Step: 234  | total loss: \u001B[1m\u001B[32m0.05361\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 234 | loss: 0.05361 - acc: 0.9934 -- iter: 3/3\n",
      "--\n",
      "Training Step: 235  | total loss: \u001B[1m\u001B[32m0.05110\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 235 | loss: 0.05110 - acc: 0.9941 -- iter: 3/3\n",
      "--\n",
      "Training Step: 236  | total loss: \u001B[1m\u001B[32m0.04882\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 236 | loss: 0.04882 - acc: 0.9947 -- iter: 3/3\n",
      "--\n",
      "Training Step: 237  | total loss: \u001B[1m\u001B[32m0.04673\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 237 | loss: 0.04673 - acc: 0.9952 -- iter: 3/3\n",
      "--\n",
      "Training Step: 238  | total loss: \u001B[1m\u001B[32m0.04481\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 238 | loss: 0.04481 - acc: 0.9957 -- iter: 3/3\n",
      "--\n",
      "Training Step: 239  | total loss: \u001B[1m\u001B[32m0.04306\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 239 | loss: 0.04306 - acc: 0.9961 -- iter: 3/3\n",
      "--\n",
      "Training Step: 240  | total loss: \u001B[1m\u001B[32m0.04144\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 240 | loss: 0.04144 - acc: 0.9965 -- iter: 3/3\n",
      "--\n",
      "Training Step: 241  | total loss: \u001B[1m\u001B[32m0.03995\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 241 | loss: 0.03995 - acc: 0.9968 -- iter: 3/3\n",
      "--\n",
      "Training Step: 242  | total loss: \u001B[1m\u001B[32m0.03858\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 242 | loss: 0.03858 - acc: 0.9972 -- iter: 3/3\n",
      "--\n",
      "Training Step: 243  | total loss: \u001B[1m\u001B[32m0.03731\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 243 | loss: 0.03731 - acc: 0.9974 -- iter: 3/3\n",
      "--\n",
      "Training Step: 244  | total loss: \u001B[1m\u001B[32m0.03614\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 244 | loss: 0.03614 - acc: 0.9977 -- iter: 3/3\n",
      "--\n",
      "Training Step: 245  | total loss: \u001B[1m\u001B[32m0.03505\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 245 | loss: 0.03505 - acc: 0.9979 -- iter: 3/3\n",
      "--\n",
      "Training Step: 246  | total loss: \u001B[1m\u001B[32m0.03404\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 246 | loss: 0.03404 - acc: 0.9981 -- iter: 3/3\n",
      "--\n",
      "Training Step: 247  | total loss: \u001B[1m\u001B[32m0.03310\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 247 | loss: 0.03310 - acc: 0.9983 -- iter: 3/3\n",
      "--\n",
      "Training Step: 248  | total loss: \u001B[1m\u001B[32m0.03221\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 248 | loss: 0.03221 - acc: 0.9985 -- iter: 3/3\n",
      "--\n",
      "Training Step: 249  | total loss: \u001B[1m\u001B[32m0.03139\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 249 | loss: 0.03139 - acc: 0.9986 -- iter: 3/3\n",
      "--\n",
      "Training Step: 250  | total loss: \u001B[1m\u001B[32m0.03062\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 250 | loss: 0.03062 - acc: 0.9988 -- iter: 3/3\n",
      "--\n",
      "Training Step: 251  | total loss: \u001B[1m\u001B[32m0.02989\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 251 | loss: 0.02989 - acc: 0.9989 -- iter: 3/3\n",
      "--\n",
      "Training Step: 252  | total loss: \u001B[1m\u001B[32m0.02921\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 252 | loss: 0.02921 - acc: 0.9990 -- iter: 3/3\n",
      "--\n",
      "Training Step: 253  | total loss: \u001B[1m\u001B[32m0.02856\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 253 | loss: 0.02856 - acc: 0.9991 -- iter: 3/3\n",
      "--\n",
      "Training Step: 254  | total loss: \u001B[1m\u001B[32m0.02795\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 254 | loss: 0.02795 - acc: 0.9992 -- iter: 3/3\n",
      "--\n",
      "Training Step: 255  | total loss: \u001B[1m\u001B[32m0.02737\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 255 | loss: 0.02737 - acc: 0.9993 -- iter: 3/3\n",
      "--\n",
      "Training Step: 256  | total loss: \u001B[1m\u001B[32m0.02682\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 256 | loss: 0.02682 - acc: 0.9994 -- iter: 3/3\n",
      "--\n",
      "Training Step: 257  | total loss: \u001B[1m\u001B[32m0.02630\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 257 | loss: 0.02630 - acc: 0.9994 -- iter: 3/3\n",
      "--\n",
      "Training Step: 258  | total loss: \u001B[1m\u001B[32m0.02580\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 258 | loss: 0.02580 - acc: 0.9995 -- iter: 3/3\n",
      "--\n",
      "Training Step: 259  | total loss: \u001B[1m\u001B[32m0.02533\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 259 | loss: 0.02533 - acc: 0.9995 -- iter: 3/3\n",
      "--\n",
      "Training Step: 260  | total loss: \u001B[1m\u001B[32m0.02487\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 260 | loss: 0.02487 - acc: 0.9996 -- iter: 3/3\n",
      "--\n",
      "Training Step: 261  | total loss: \u001B[1m\u001B[32m0.02444\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 261 | loss: 0.02444 - acc: 0.9996 -- iter: 3/3\n",
      "--\n",
      "Training Step: 262  | total loss: \u001B[1m\u001B[32m0.02402\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 262 | loss: 0.02402 - acc: 0.9997 -- iter: 3/3\n",
      "--\n",
      "Training Step: 263  | total loss: \u001B[1m\u001B[32m0.02362\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 263 | loss: 0.02362 - acc: 0.9997 -- iter: 3/3\n",
      "--\n",
      "Training Step: 264  | total loss: \u001B[1m\u001B[32m0.02323\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 264 | loss: 0.02323 - acc: 0.9997 -- iter: 3/3\n",
      "--\n",
      "Training Step: 265  | total loss: \u001B[1m\u001B[32m0.02286\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 265 | loss: 0.02286 - acc: 0.9997 -- iter: 3/3\n",
      "--\n",
      "Training Step: 266  | total loss: \u001B[1m\u001B[32m0.02250\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 266 | loss: 0.02250 - acc: 0.9998 -- iter: 3/3\n",
      "--\n",
      "Training Step: 267  | total loss: \u001B[1m\u001B[32m0.02215\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 267 | loss: 0.02215 - acc: 0.9998 -- iter: 3/3\n",
      "--\n",
      "Training Step: 268  | total loss: \u001B[1m\u001B[32m0.02182\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 268 | loss: 0.02182 - acc: 0.9998 -- iter: 3/3\n",
      "--\n",
      "Training Step: 269  | total loss: \u001B[1m\u001B[32m0.02149\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 269 | loss: 0.02149 - acc: 0.9998 -- iter: 3/3\n",
      "--\n",
      "Training Step: 270  | total loss: \u001B[1m\u001B[32m0.02118\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 270 | loss: 0.02118 - acc: 0.9999 -- iter: 3/3\n",
      "--\n",
      "Training Step: 271  | total loss: \u001B[1m\u001B[32m0.02087\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 271 | loss: 0.02087 - acc: 0.9999 -- iter: 3/3\n",
      "--\n",
      "Training Step: 272  | total loss: \u001B[1m\u001B[32m0.02058\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 272 | loss: 0.02058 - acc: 0.9999 -- iter: 3/3\n",
      "--\n",
      "Training Step: 273  | total loss: \u001B[1m\u001B[32m0.02029\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 273 | loss: 0.02029 - acc: 0.9999 -- iter: 3/3\n",
      "--\n",
      "Training Step: 274  | total loss: \u001B[1m\u001B[32m0.02001\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 274 | loss: 0.02001 - acc: 0.9999 -- iter: 3/3\n",
      "--\n",
      "Training Step: 275  | total loss: \u001B[1m\u001B[32m0.01973\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 275 | loss: 0.01973 - acc: 0.9999 -- iter: 3/3\n",
      "--\n",
      "Training Step: 276  | total loss: \u001B[1m\u001B[32m0.01947\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 276 | loss: 0.01947 - acc: 0.9999 -- iter: 3/3\n",
      "--\n",
      "Training Step: 277  | total loss: \u001B[1m\u001B[32m0.01921\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 277 | loss: 0.01921 - acc: 0.9999 -- iter: 3/3\n",
      "--\n",
      "Training Step: 278  | total loss: \u001B[1m\u001B[32m0.01895\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 278 | loss: 0.01895 - acc: 0.9999 -- iter: 3/3\n",
      "--\n",
      "Training Step: 279  | total loss: \u001B[1m\u001B[32m0.01871\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 279 | loss: 0.01871 - acc: 0.9999 -- iter: 3/3\n",
      "--\n",
      "Training Step: 280  | total loss: \u001B[1m\u001B[32m0.01846\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 280 | loss: 0.01846 - acc: 0.9999 -- iter: 3/3\n",
      "--\n",
      "Training Step: 281  | total loss: \u001B[1m\u001B[32m0.01823\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 281 | loss: 0.01823 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 282  | total loss: \u001B[1m\u001B[32m0.01800\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 282 | loss: 0.01800 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 283  | total loss: \u001B[1m\u001B[32m0.01777\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 283 | loss: 0.01777 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 284  | total loss: \u001B[1m\u001B[32m0.01755\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 284 | loss: 0.01755 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 285  | total loss: \u001B[1m\u001B[32m0.01733\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 285 | loss: 0.01733 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 286  | total loss: \u001B[1m\u001B[32m0.01712\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 286 | loss: 0.01712 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 287  | total loss: \u001B[1m\u001B[32m0.01691\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 287 | loss: 0.01691 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 288  | total loss: \u001B[1m\u001B[32m0.01671\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 288 | loss: 0.01671 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 289  | total loss: \u001B[1m\u001B[32m0.01651\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 289 | loss: 0.01651 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 290  | total loss: \u001B[1m\u001B[32m0.01632\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 290 | loss: 0.01632 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 291  | total loss: \u001B[1m\u001B[32m0.01613\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 291 | loss: 0.01613 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 292  | total loss: \u001B[1m\u001B[32m0.01594\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 292 | loss: 0.01594 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 293  | total loss: \u001B[1m\u001B[32m0.01575\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 293 | loss: 0.01575 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 294  | total loss: \u001B[1m\u001B[32m0.01557\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 294 | loss: 0.01557 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 295  | total loss: \u001B[1m\u001B[32m0.01539\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 295 | loss: 0.01539 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 296  | total loss: \u001B[1m\u001B[32m0.01522\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 296 | loss: 0.01522 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 297  | total loss: \u001B[1m\u001B[32m0.01505\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 297 | loss: 0.01505 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 298  | total loss: \u001B[1m\u001B[32m0.01488\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 298 | loss: 0.01488 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 299  | total loss: \u001B[1m\u001B[32m0.01472\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 299 | loss: 0.01472 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 300  | total loss: \u001B[1m\u001B[32m0.01455\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 300 | loss: 0.01455 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 301  | total loss: \u001B[1m\u001B[32m0.01439\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 301 | loss: 0.01439 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 302  | total loss: \u001B[1m\u001B[32m0.01424\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 302 | loss: 0.01424 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 303  | total loss: \u001B[1m\u001B[32m0.01408\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 303 | loss: 0.01408 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 304  | total loss: \u001B[1m\u001B[32m0.01393\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 304 | loss: 0.01393 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 305  | total loss: \u001B[1m\u001B[32m0.01378\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 305 | loss: 0.01378 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 306  | total loss: \u001B[1m\u001B[32m0.01364\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 306 | loss: 0.01364 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 307  | total loss: \u001B[1m\u001B[32m0.01349\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 307 | loss: 0.01349 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 308  | total loss: \u001B[1m\u001B[32m0.01335\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 308 | loss: 0.01335 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 309  | total loss: \u001B[1m\u001B[32m0.01321\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 309 | loss: 0.01321 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 310  | total loss: \u001B[1m\u001B[32m0.01308\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 310 | loss: 0.01308 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 311  | total loss: \u001B[1m\u001B[32m0.01294\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 311 | loss: 0.01294 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 312  | total loss: \u001B[1m\u001B[32m0.01281\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 312 | loss: 0.01281 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 313  | total loss: \u001B[1m\u001B[32m0.01268\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 313 | loss: 0.01268 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 314  | total loss: \u001B[1m\u001B[32m0.01255\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 314 | loss: 0.01255 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 315  | total loss: \u001B[1m\u001B[32m0.01242\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 315 | loss: 0.01242 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 316  | total loss: \u001B[1m\u001B[32m0.01230\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 316 | loss: 0.01230 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 317  | total loss: \u001B[1m\u001B[32m0.01218\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 317 | loss: 0.01218 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 318  | total loss: \u001B[1m\u001B[32m0.01205\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 318 | loss: 0.01205 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 319  | total loss: \u001B[1m\u001B[32m0.01194\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 319 | loss: 0.01194 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 320  | total loss: \u001B[1m\u001B[32m0.01182\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 320 | loss: 0.01182 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 321  | total loss: \u001B[1m\u001B[32m0.01170\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 321 | loss: 0.01170 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 322  | total loss: \u001B[1m\u001B[32m0.01159\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 322 | loss: 0.01159 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 323  | total loss: \u001B[1m\u001B[32m0.01148\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 323 | loss: 0.01148 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 324  | total loss: \u001B[1m\u001B[32m0.01137\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 324 | loss: 0.01137 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 325  | total loss: \u001B[1m\u001B[32m0.01126\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 325 | loss: 0.01126 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 326  | total loss: \u001B[1m\u001B[32m0.01115\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 326 | loss: 0.01115 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 327  | total loss: \u001B[1m\u001B[32m0.01105\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 327 | loss: 0.01105 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 328  | total loss: \u001B[1m\u001B[32m0.01094\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 328 | loss: 0.01094 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 329  | total loss: \u001B[1m\u001B[32m0.01084\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 329 | loss: 0.01084 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 330  | total loss: \u001B[1m\u001B[32m0.01074\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 330 | loss: 0.01074 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 331  | total loss: \u001B[1m\u001B[32m0.01064\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 331 | loss: 0.01064 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 332  | total loss: \u001B[1m\u001B[32m0.01054\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 332 | loss: 0.01054 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 333  | total loss: \u001B[1m\u001B[32m0.01045\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 333 | loss: 0.01045 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 334  | total loss: \u001B[1m\u001B[32m0.01035\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 334 | loss: 0.01035 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 335  | total loss: \u001B[1m\u001B[32m0.01026\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 335 | loss: 0.01026 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 336  | total loss: \u001B[1m\u001B[32m0.01017\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 336 | loss: 0.01017 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 337  | total loss: \u001B[1m\u001B[32m0.01007\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 337 | loss: 0.01007 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 338  | total loss: \u001B[1m\u001B[32m0.00998\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 338 | loss: 0.00998 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 339  | total loss: \u001B[1m\u001B[32m0.00989\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 339 | loss: 0.00989 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 340  | total loss: \u001B[1m\u001B[32m0.00981\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 340 | loss: 0.00981 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 341  | total loss: \u001B[1m\u001B[32m0.00972\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 341 | loss: 0.00972 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 342  | total loss: \u001B[1m\u001B[32m0.00964\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 342 | loss: 0.00964 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 343  | total loss: \u001B[1m\u001B[32m0.00955\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 343 | loss: 0.00955 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 344  | total loss: \u001B[1m\u001B[32m0.00947\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 344 | loss: 0.00947 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 345  | total loss: \u001B[1m\u001B[32m0.00939\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 345 | loss: 0.00939 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 346  | total loss: \u001B[1m\u001B[32m0.00931\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 346 | loss: 0.00931 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 347  | total loss: \u001B[1m\u001B[32m0.00923\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 347 | loss: 0.00923 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 348  | total loss: \u001B[1m\u001B[32m0.00915\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 348 | loss: 0.00915 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 349  | total loss: \u001B[1m\u001B[32m0.00907\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 349 | loss: 0.00907 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 350  | total loss: \u001B[1m\u001B[32m0.00899\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 350 | loss: 0.00899 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 351  | total loss: \u001B[1m\u001B[32m0.00892\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 351 | loss: 0.00892 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 352  | total loss: \u001B[1m\u001B[32m0.00884\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 352 | loss: 0.00884 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 353  | total loss: \u001B[1m\u001B[32m0.00877\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 353 | loss: 0.00877 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 354  | total loss: \u001B[1m\u001B[32m0.00870\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 354 | loss: 0.00870 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 355  | total loss: \u001B[1m\u001B[32m0.00863\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 355 | loss: 0.00863 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 356  | total loss: \u001B[1m\u001B[32m0.00855\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 356 | loss: 0.00855 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 357  | total loss: \u001B[1m\u001B[32m0.00848\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 357 | loss: 0.00848 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 358  | total loss: \u001B[1m\u001B[32m0.00842\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 358 | loss: 0.00842 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 359  | total loss: \u001B[1m\u001B[32m0.00835\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 359 | loss: 0.00835 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 360  | total loss: \u001B[1m\u001B[32m0.00828\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 360 | loss: 0.00828 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 361  | total loss: \u001B[1m\u001B[32m0.00821\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 361 | loss: 0.00821 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 362  | total loss: \u001B[1m\u001B[32m0.00815\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 362 | loss: 0.00815 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 363  | total loss: \u001B[1m\u001B[32m0.00808\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 363 | loss: 0.00808 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 364  | total loss: \u001B[1m\u001B[32m0.00802\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 364 | loss: 0.00802 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 365  | total loss: \u001B[1m\u001B[32m0.00796\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 365 | loss: 0.00796 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 366  | total loss: \u001B[1m\u001B[32m0.00789\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 366 | loss: 0.00789 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 367  | total loss: \u001B[1m\u001B[32m0.00783\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 367 | loss: 0.00783 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 368  | total loss: \u001B[1m\u001B[32m0.00777\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 368 | loss: 0.00777 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 369  | total loss: \u001B[1m\u001B[32m0.00771\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 369 | loss: 0.00771 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 370  | total loss: \u001B[1m\u001B[32m0.00765\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 370 | loss: 0.00765 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 371  | total loss: \u001B[1m\u001B[32m0.00759\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 371 | loss: 0.00759 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 372  | total loss: \u001B[1m\u001B[32m0.00753\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 372 | loss: 0.00753 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 373  | total loss: \u001B[1m\u001B[32m0.00748\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 373 | loss: 0.00748 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 374  | total loss: \u001B[1m\u001B[32m0.00742\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 374 | loss: 0.00742 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 375  | total loss: \u001B[1m\u001B[32m0.00736\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 375 | loss: 0.00736 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 376  | total loss: \u001B[1m\u001B[32m0.00731\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 376 | loss: 0.00731 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 377  | total loss: \u001B[1m\u001B[32m0.00725\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 377 | loss: 0.00725 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 378  | total loss: \u001B[1m\u001B[32m0.00720\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 378 | loss: 0.00720 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 379  | total loss: \u001B[1m\u001B[32m0.00714\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 379 | loss: 0.00714 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 380  | total loss: \u001B[1m\u001B[32m0.00709\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 380 | loss: 0.00709 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 381  | total loss: \u001B[1m\u001B[32m0.00704\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 381 | loss: 0.00704 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 382  | total loss: \u001B[1m\u001B[32m0.00699\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 382 | loss: 0.00699 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 383  | total loss: \u001B[1m\u001B[32m0.00694\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 383 | loss: 0.00694 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 384  | total loss: \u001B[1m\u001B[32m0.00688\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 384 | loss: 0.00688 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 385  | total loss: \u001B[1m\u001B[32m0.00683\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 385 | loss: 0.00683 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 386  | total loss: \u001B[1m\u001B[32m0.00679\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 386 | loss: 0.00679 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 387  | total loss: \u001B[1m\u001B[32m0.00674\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 387 | loss: 0.00674 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 388  | total loss: \u001B[1m\u001B[32m0.00669\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 388 | loss: 0.00669 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 389  | total loss: \u001B[1m\u001B[32m0.00664\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 389 | loss: 0.00664 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 390  | total loss: \u001B[1m\u001B[32m0.00659\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 390 | loss: 0.00659 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 391  | total loss: \u001B[1m\u001B[32m0.00655\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 391 | loss: 0.00655 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 392  | total loss: \u001B[1m\u001B[32m0.00650\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 392 | loss: 0.00650 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 393  | total loss: \u001B[1m\u001B[32m0.00645\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 393 | loss: 0.00645 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 394  | total loss: \u001B[1m\u001B[32m0.00641\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 394 | loss: 0.00641 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 395  | total loss: \u001B[1m\u001B[32m0.00636\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 395 | loss: 0.00636 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 396  | total loss: \u001B[1m\u001B[32m0.00632\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 396 | loss: 0.00632 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 397  | total loss: \u001B[1m\u001B[32m0.00628\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 397 | loss: 0.00628 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 398  | total loss: \u001B[1m\u001B[32m0.00623\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 398 | loss: 0.00623 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 399  | total loss: \u001B[1m\u001B[32m0.00619\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 399 | loss: 0.00619 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 400  | total loss: \u001B[1m\u001B[32m0.00615\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 400 | loss: 0.00615 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 401  | total loss: \u001B[1m\u001B[32m0.00610\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 401 | loss: 0.00610 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 402  | total loss: \u001B[1m\u001B[32m0.00606\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 402 | loss: 0.00606 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 403  | total loss: \u001B[1m\u001B[32m0.00602\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 403 | loss: 0.00602 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 404  | total loss: \u001B[1m\u001B[32m0.00598\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 404 | loss: 0.00598 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 405  | total loss: \u001B[1m\u001B[32m0.00594\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 405 | loss: 0.00594 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 406  | total loss: \u001B[1m\u001B[32m0.00590\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 406 | loss: 0.00590 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 407  | total loss: \u001B[1m\u001B[32m0.00586\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 407 | loss: 0.00586 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 408  | total loss: \u001B[1m\u001B[32m0.00582\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 408 | loss: 0.00582 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 409  | total loss: \u001B[1m\u001B[32m0.00578\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 409 | loss: 0.00578 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 410  | total loss: \u001B[1m\u001B[32m0.00574\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 410 | loss: 0.00574 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 411  | total loss: \u001B[1m\u001B[32m0.00571\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 411 | loss: 0.00571 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 412  | total loss: \u001B[1m\u001B[32m0.00567\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 412 | loss: 0.00567 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 413  | total loss: \u001B[1m\u001B[32m0.00563\u001B[0m\u001B[0m | time: 0.003s\n",
      "| Adam | epoch: 413 | loss: 0.00563 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 414  | total loss: \u001B[1m\u001B[32m0.00560\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 414 | loss: 0.00560 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 415  | total loss: \u001B[1m\u001B[32m0.00556\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 415 | loss: 0.00556 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 416  | total loss: \u001B[1m\u001B[32m0.00552\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 416 | loss: 0.00552 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 417  | total loss: \u001B[1m\u001B[32m0.00549\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 417 | loss: 0.00549 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 418  | total loss: \u001B[1m\u001B[32m0.00545\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 418 | loss: 0.00545 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 419  | total loss: \u001B[1m\u001B[32m0.00542\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 419 | loss: 0.00542 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 420  | total loss: \u001B[1m\u001B[32m0.00538\u001B[0m\u001B[0m | time: 0.001s\n",
      "| Adam | epoch: 420 | loss: 0.00538 - acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "INFO:tensorflow:/Users/danielkrasovski/Documents/GitHub/Discord_AI_Bot/models/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "tf.compat.v1.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=420, batch_size=8, show_metric=True)\n",
    "model.save('models/model.tflearn')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"models/training_data\", \"wb\" ) )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% save all of our data structures\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}